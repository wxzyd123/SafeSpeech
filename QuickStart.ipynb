{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"bert_vits2/\")\n",
    "\n",
    "import bert_vits2.utils as utils\n",
    "from bert_vits2.data_utils import (\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextAudioSpeakerCollate,\n",
    ")\n",
    "from bert_vits2.losses import WavLMLoss\n",
    "from toolbox import build_models_noise, build_optims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bert_gen.py --mode clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m12-24 13:01:34\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:61 | Init dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:00<00:00, 35777.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m12-24 13:01:34\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:76 | skipped: 0, total: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_name = \"BERT_VITS2\"\n",
    "dataset_name = \"LibriTTS\"\n",
    "mode = \"SPEC\"\n",
    "\n",
    "config_path = f\"bert_vits2/configs/{dataset_name.lower()}_{model_name.lower()}.json\"\n",
    "hps = utils.get_hparams_from_file(config_path)\n",
    "hps.train.batch_size = 27\n",
    "hps.model_dir = \"/root/autodl-tmp/SafeSpeech/checkpoints/base_models\"\n",
    "\n",
    "torch.manual_seed(hps.train.seed)\n",
    "torch.cuda.manual_seed(hps.train.seed)\n",
    "train_dataset = TextAudioSpeakerLoader(hps.data.training_files, hps.data)\n",
    "collate_fn = TextAudioSpeakerCollate()\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            num_workers=4,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=collate_fn,\n",
    "                            batch_size=hps.train.batch_size,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. initialize models and Generate perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:bert_vits2.utils:emb_g.weight is not in the checkpoint\n"
     ]
    }
   ],
   "source": [
    "### Build models and optimizers\n",
    "nets = build_models_noise(hps, device)\n",
    "net_g, net_d, net_wd, net_dur_disc = nets\n",
    "\n",
    "optims = build_optims(hps, nets)\n",
    "optim_g, optim_d, optim_wd, optim_dur_disc = optims\n",
    "\n",
    "dur_resume_lr = hps.train.learning_rate\n",
    "wd_resume_lr = hps.train.learning_rate\n",
    "\n",
    "_, _, dur_resume_lr, epoch_str = utils.load_checkpoint(\n",
    "    utils.latest_checkpoint_path(hps.model_dir, \"DUR_*.pth\"),\n",
    "    net_dur_disc,\n",
    "    optim_dur_disc,\n",
    "    skip_optimizer=(\n",
    "        hps.train.skip_optimizer if \"skip_optimizer\" in hps.train else True\n",
    "    ),\n",
    ")\n",
    "if not optim_dur_disc.param_groups[0].get(\"initial_lr\"):\n",
    "    optim_dur_disc.param_groups[0][\"initial_lr\"] = dur_resume_lr\n",
    "\n",
    "_, optim_g, g_resume_lr, epoch_str = utils.load_checkpoint(\n",
    "    utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\"),\n",
    "    net_g,\n",
    "    optim_g,\n",
    "    skip_optimizer=(\n",
    "        hps.train.skip_optimizer if \"skip_optimizer\" in hps.train else True\n",
    "    ),\n",
    ")\n",
    "_, optim_d, d_resume_lr, epoch_str = utils.load_checkpoint(\n",
    "    utils.latest_checkpoint_path(hps.model_dir, \"D_*.pth\"),\n",
    "    net_d,\n",
    "    optim_d,\n",
    "    skip_optimizer=(\n",
    "        hps.train.skip_optimizer if \"skip_optimizer\" in hps.train else True\n",
    "    ),\n",
    ")\n",
    "if not optim_g.param_groups[0].get(\"initial_lr\"):\n",
    "    optim_g.param_groups[0][\"initial_lr\"] = g_resume_lr\n",
    "if not optim_d.param_groups[0].get(\"initial_lr\"):\n",
    "    optim_d.param_groups[0][\"initial_lr\"] = d_resume_lr\n",
    "\n",
    "epoch_str = max(epoch_str, 1)\n",
    "global_step = int(utils.get_steps(utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\")))\n",
    "\n",
    "_, optim_wd, wd_resume_lr, epoch_str = utils.load_checkpoint(\n",
    "    utils.latest_checkpoint_path(hps.model_dir, \"WD_*.pth\"),\n",
    "    net_wd,\n",
    "    optim_wd,\n",
    "    skip_optimizer=(\n",
    "        hps.train.skip_optimizer if \"skip_optimizer\" in hps.train else True\n",
    "    ),\n",
    ")\n",
    "if not optim_wd.param_groups[0].get(\"initial_lr\"):\n",
    "    optim_wd.param_groups[0][\"initial_lr\"] = wd_resume_lr\n",
    "\n",
    "wl = WavLMLoss(\n",
    "    hps.model.slm.model,\n",
    "    net_wd,\n",
    "    hps.data.sampling_rate,\n",
    "    hps.model.slm.sr,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss {'loss_mel': '18.554979', 'loss_nr': '172.343292', 'loss_kl': '18.774075'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:04<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Loss {'loss_mel': '18.611267', 'loss_nr': '170.562912', 'loss_kl': '18.865513'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:12<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2: Loss {'loss_mel': '18.689875', 'loss_nr': '172.696991', 'loss_kl': '18.187258'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:05<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3: Loss {'loss_mel': '18.502092', 'loss_nr': '175.578339', 'loss_kl': '19.883799'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from protect import perturb\n",
    "\n",
    "noises = [None] * len(train_loader)\n",
    "max_epoch = 200\n",
    "epsilon = 8 / 255\n",
    "alpha = epsilon / 10\n",
    "\n",
    "for param in net_g.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in net_d.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in net_dur_disc.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in net_wd.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in wl.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "### Begin to generate perturbation...\n",
    "for batch_index, batch_data in enumerate(train_loader):\n",
    "    loss, noises[batch_index] = perturb(hps, [net_g, _, _, _, _], batch_data, \n",
    "                                        epsilon, alpha, max_epoch, 10, device)\n",
    "\n",
    "    print(f\"Batch {batch_index}: Loss {loss}\")\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save the noise to ./checkpoints/LibriTTS/noises/BERT_VITS2_SPEC_LibriTTS.noise!\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"./checkpoints/{dataset_name}/noises/\", exist_ok=True)\n",
    "noise_save_path = f\"./checkpoints/{dataset_name}/noises/{model_name}_{mode}_{dataset_name}.noise\"\n",
    "torch.save(noises, noise_save_path)\n",
    "print(f\"Save the noise to {noise_save_path}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Save audio and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m12-24 13:15:55\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:61 | Init dataset...\n",
      "100%|██████████████████████████████████████| 108/108 [00:00<00:00, 40165.35it/s]\n",
      "\u001b[32m12-24 13:15:55\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:76 | skipped: 0, total: 108\n",
      "The noise path is checkpoints/LibriTTS/noises/BERT_VITS2_SPEC_LibriTTS.noise\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Save audio\n",
    "!python save_audio.py --mode SPEC --model BERT_VITS2 --dataset LibriTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python bert_gen.py --mode SPEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train models on the protected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m12-24 13:16:27\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:61 | Init dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:00<00:00, 18512.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m12-24 13:16:27\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:76 | skipped: 0, total: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hps.train.batch_size = 64\n",
    "if mode != \"clean\":\n",
    "    hps.data.training_files = \"filelists/libritts_train_asr.txt.cleaned\"\n",
    "hps.model_dir = \"checkpoints/base_models\"\n",
    "assert os.listdir(hps.model_dir) != 4\n",
    "\n",
    "global global_step\n",
    "torch.manual_seed(hps.train.seed)\n",
    "train_dataset = TextAudioSpeakerLoader(hps.data.training_files, hps.data)\n",
    "collate_fn = TextAudioSpeakerCollate()\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=4,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=hps.train.batch_size,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:bert_vits2.utils:emb_g.weight is not in the checkpoint\n"
     ]
    }
   ],
   "source": [
    "from toolbox import build_models, build_optims, build_schedulers\n",
    "\n",
    "models = build_models(hps, device)\n",
    "net_g, net_d, net_wd, net_dur_disc = models\n",
    "\n",
    "optims = build_optims(hps, models)\n",
    "optim_g, optim_d, optim_wd, optim_dur_disc = optims\n",
    "\n",
    "dur_resume_lr = hps.train.learning_rate\n",
    "wd_resume_lr = hps.train.learning_rate\n",
    "_, _, dur_resume_lr, epoch_str = utils.load_checkpoint(\n",
    "    utils.latest_checkpoint_path(hps.model_dir, \"DUR_*.pth\"),\n",
    "    net_dur_disc,\n",
    "    optim_dur_disc,\n",
    "    skip_optimizer=(\n",
    "        hps.train.skip_optimizer if \"skip_optimizer\" in hps.train else True\n",
    "    ),\n",
    ")\n",
    "if not optim_dur_disc.param_groups[0].get(\"initial_lr\"):\n",
    "    optim_dur_disc.param_groups[0][\"initial_lr\"] = dur_resume_lr\n",
    "\n",
    "_, optim_g, g_resume_lr, epoch_str = utils.load_checkpoint(\n",
    "    utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\"),\n",
    "    net_g,\n",
    "    optim_g,\n",
    "    skip_optimizer=(\n",
    "        hps.train.skip_optimizer if \"skip_optimizer\" in hps.train else True\n",
    "    ),\n",
    ")\n",
    "_, optim_d, d_resume_lr, epoch_str = utils.load_checkpoint(\n",
    "    utils.latest_checkpoint_path(hps.model_dir, \"D_*.pth\"),\n",
    "    net_d,\n",
    "    optim_d,\n",
    "    skip_optimizer=(\n",
    "        hps.train.skip_optimizer if \"skip_optimizer\" in hps.train else True\n",
    "    ),\n",
    ")\n",
    "if not optim_g.param_groups[0].get(\"initial_lr\"):\n",
    "    optim_g.param_groups[0][\"initial_lr\"] = g_resume_lr\n",
    "if not optim_d.param_groups[0].get(\"initial_lr\"):\n",
    "    optim_d.param_groups[0][\"initial_lr\"] = d_resume_lr\n",
    "\n",
    "epoch_str = max(epoch_str, 1)\n",
    "global_step = int(utils.get_steps(utils.latest_checkpoint_path(hps.model_dir, \"G_*.pth\")))\n",
    "\n",
    "_, optim_wd, wd_resume_lr, epoch_str = utils.load_checkpoint(\n",
    "    utils.latest_checkpoint_path(hps.model_dir, \"WD_*.pth\"),\n",
    "    net_wd,\n",
    "    optim_wd,\n",
    "    skip_optimizer=(\n",
    "        hps.train.skip_optimizer if \"skip_optimizer\" in hps.train else True\n",
    "    ),\n",
    ")\n",
    "if not optim_wd.param_groups[0].get(\"initial_lr\"):\n",
    "    optim_wd.param_groups[0][\"initial_lr\"] = wd_resume_lr\n",
    "\n",
    "\n",
    "schedulers = build_schedulers(hps, optims, epoch_str)\n",
    "scheduler_g, scheduler_d, scheduler_wd, scheduler_dur_disc = schedulers\n",
    "\n",
    "scaler = GradScaler(enabled=hps.train.bf16_run)\n",
    "\n",
    "wl = WavLMLoss(\n",
    "    hps.model.slm.model,\n",
    "    net_wd,\n",
    "    hps.data.sampling_rate,\n",
    "    hps.model.slm.sr,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:05] Epoch 1: G 93.337967, D 4.087115 Dur 0.006502, Sim 0.151331\n",
      "[00:00:10] Epoch 2: G 88.848785, D 3.143491 Dur 0.002086, Sim 0.072668\n",
      "[00:00:14] Epoch 3: G 87.998581, D 3.001774 Dur 0.000998, Sim 0.173233\n",
      "[00:00:19] Epoch 4: G 89.167023, D 2.664698 Dur 0.001019, Sim 0.093984\n",
      "[00:00:24] Epoch 5: G 88.649010, D 2.619440 Dur 0.000481, Sim 0.098045\n",
      "[00:00:28] Epoch 6: G 87.663193, D 2.584833 Dur 0.000377, Sim 0.118725\n",
      "[00:00:32] Epoch 7: G 86.267006, D 2.621395 Dur 0.000244, Sim 0.086182\n",
      "[00:00:37] Epoch 8: G 84.361595, D 2.902445 Dur 0.000157, Sim 0.067962\n",
      "[00:00:41] Epoch 9: G 85.292183, D 2.858322 Dur 0.000112, Sim 0.051358\n",
      "[00:00:46] Epoch 10: G 83.716187, D 2.767478 Dur 0.000097, Sim 0.095324\n",
      "[00:00:50] Epoch 11: G 85.001938, D 2.339326 Dur 0.000045, Sim 0.058034\n",
      "[00:00:55] Epoch 12: G 84.627625, D 2.229191 Dur 0.000061, Sim 0.062847\n",
      "[00:00:59] Epoch 13: G 83.859947, D 2.435069 Dur 0.000020, Sim 0.058307\n",
      "[00:01:04] Epoch 14: G 82.848976, D 2.857036 Dur 0.000040, Sim 0.088750\n",
      "[00:01:08] Epoch 15: G 83.712082, D 2.717084 Dur 0.000017, Sim 0.070649\n",
      "[00:01:12] Epoch 16: G 82.460968, D 2.534470 Dur 0.000021, Sim 0.122139\n",
      "[00:01:17] Epoch 17: G 82.997498, D 2.477955 Dur 0.000019, Sim 0.065758\n",
      "[00:01:21] Epoch 18: G 84.382828, D 2.395719 Dur 0.000017, Sim 0.036083\n",
      "[00:01:26] Epoch 19: G 84.035919, D 2.409662 Dur 0.000019, Sim 0.057567\n",
      "[00:01:30] Epoch 20: G 83.988617, D 2.487408 Dur 0.000016, Sim 0.040990\n",
      "[00:01:35] Epoch 21: G 83.672485, D 2.525253 Dur 0.000014, Sim 0.063291\n",
      "[00:01:39] Epoch 22: G 83.773376, D 2.435637 Dur 0.000014, Sim 0.061278\n",
      "[00:01:44] Epoch 23: G 82.684586, D 2.439138 Dur 0.000016, Sim 0.085677\n",
      "[00:01:48] Epoch 24: G 82.466690, D 2.441801 Dur 0.000014, Sim 0.099012\n",
      "[00:01:53] Epoch 25: G 83.121475, D 2.416803 Dur 0.000015, Sim 0.040780\n",
      "[00:01:57] Epoch 26: G 81.905624, D 2.473107 Dur 0.000014, Sim 0.044129\n",
      "[00:02:02] Epoch 27: G 81.514191, D 2.463274 Dur 0.000014, Sim 0.107845\n",
      "[00:02:06] Epoch 28: G 81.611053, D 2.589483 Dur 0.000013, Sim 0.090535\n",
      "[00:02:11] Epoch 29: G 81.635178, D 2.640553 Dur 0.000014, Sim 0.065869\n",
      "[00:02:15] Epoch 30: G 82.343056, D 2.691471 Dur 0.000013, Sim 0.071924\n",
      "[00:02:20] Epoch 31: G 82.000443, D 2.691600 Dur 0.000014, Sim 0.071866\n",
      "[00:02:24] Epoch 32: G 82.412552, D 2.523386 Dur 0.000013, Sim 0.072764\n",
      "[00:02:29] Epoch 33: G 82.971169, D 2.217699 Dur 0.000012, Sim 0.083852\n",
      "[00:02:33] Epoch 34: G 81.846527, D 2.407873 Dur 0.000014, Sim 0.133990\n",
      "[00:02:38] Epoch 35: G 82.937851, D 2.577169 Dur 0.000012, Sim 0.094632\n",
      "[00:02:42] Epoch 36: G 82.359848, D 2.645169 Dur 0.000012, Sim 0.083344\n",
      "[00:02:47] Epoch 37: G 82.421783, D 2.493700 Dur 0.000013, Sim 0.077366\n",
      "[00:02:51] Epoch 38: G 81.470085, D 2.388941 Dur 0.000012, Sim 0.077608\n",
      "[00:02:56] Epoch 39: G 81.968765, D 2.335060 Dur 0.000012, Sim 0.053030\n",
      "[00:03:00] Epoch 40: G 81.763763, D 2.264143 Dur 0.000012, Sim 0.110061\n",
      "[00:03:05] Epoch 41: G 82.158524, D 2.250705 Dur 0.000014, Sim 0.073082\n",
      "[00:03:09] Epoch 42: G 80.540802, D 2.564594 Dur 0.000013, Sim 0.097129\n",
      "[00:03:14] Epoch 43: G 82.015427, D 2.604461 Dur 0.000013, Sim 0.062463\n",
      "[00:03:18] Epoch 44: G 83.463646, D 2.434379 Dur 0.000012, Sim 0.082046\n",
      "[00:03:22] Epoch 45: G 81.741463, D 2.242977 Dur 0.000012, Sim 0.045689\n",
      "[00:03:27] Epoch 46: G 81.400604, D 2.402655 Dur 0.000011, Sim 0.076648\n",
      "[00:03:32] Epoch 47: G 81.891174, D 2.262107 Dur 0.000011, Sim 0.060334\n",
      "[00:03:36] Epoch 48: G 81.095627, D 2.218043 Dur 0.000013, Sim 0.095374\n",
      "[00:03:41] Epoch 49: G 81.424774, D 2.140073 Dur 0.000013, Sim 0.089054\n",
      "[00:03:45] Epoch 50: G 81.390747, D 2.234228 Dur 0.000012, Sim 0.091050\n",
      "[00:03:50] Epoch 51: G 82.127441, D 2.165243 Dur 0.000014, Sim 0.103907\n",
      "[00:03:54] Epoch 52: G 81.575546, D 2.290703 Dur 0.000035, Sim 0.119358\n",
      "[00:03:59] Epoch 53: G 82.422638, D 2.329846 Dur 0.000218, Sim 0.117513\n",
      "[00:04:03] Epoch 54: G 81.837692, D 2.283823 Dur 0.000668, Sim 0.123366\n",
      "[00:04:07] Epoch 55: G 81.641449, D 2.220998 Dur 0.000026, Sim 0.097398\n",
      "[00:04:12] Epoch 56: G 82.822617, D 2.252513 Dur 0.000293, Sim 0.063799\n",
      "[00:04:16] Epoch 57: G 81.885811, D 2.290610 Dur 0.000037, Sim 0.092480\n",
      "[00:04:21] Epoch 58: G 81.517807, D 2.351740 Dur 0.000090, Sim 0.080899\n",
      "[00:04:26] Epoch 59: G 81.254639, D 2.280023 Dur 0.000070, Sim 0.102019\n",
      "[00:04:30] Epoch 60: G 80.932144, D 2.238790 Dur 0.000013, Sim 0.054651\n",
      "[00:04:35] Epoch 61: G 82.201324, D 2.275290 Dur 0.000056, Sim 0.052129\n",
      "[00:04:39] Epoch 62: G 81.595261, D 2.370866 Dur 0.000013, Sim 0.084608\n",
      "[00:04:44] Epoch 63: G 81.074615, D 2.270255 Dur 0.000027, Sim 0.073617\n",
      "[00:04:48] Epoch 64: G 81.672089, D 2.159731 Dur 0.000014, Sim 0.063399\n",
      "[00:04:53] Epoch 65: G 82.454407, D 2.056603 Dur 0.000015, Sim 0.041405\n",
      "[00:04:57] Epoch 66: G 82.959900, D 2.081906 Dur 0.000017, Sim 0.066895\n",
      "[00:05:02] Epoch 67: G 82.206161, D 2.197995 Dur 0.000015, Sim 0.066394\n",
      "[00:05:06] Epoch 68: G 82.783905, D 2.320846 Dur 0.000013, Sim 0.045411\n",
      "[00:05:11] Epoch 69: G 82.872856, D 2.242124 Dur 0.000023, Sim 0.029331\n",
      "[00:05:15] Epoch 70: G 81.893852, D 2.226262 Dur 0.000016, Sim 0.054305\n",
      "[00:05:20] Epoch 71: G 82.835594, D 2.190535 Dur 0.000023, Sim 0.050635\n",
      "[00:05:24] Epoch 72: G 82.231934, D 2.196348 Dur 0.000027, Sim 0.056956\n",
      "[00:05:29] Epoch 73: G 82.433098, D 2.234553 Dur 0.000043, Sim 0.054791\n",
      "[00:05:33] Epoch 74: G 82.796715, D 2.145098 Dur 0.000060, Sim 0.064448\n",
      "[00:05:38] Epoch 75: G 82.846848, D 2.047996 Dur 0.000089, Sim 0.063840\n",
      "[00:05:42] Epoch 76: G 82.359520, D 2.192176 Dur 0.000131, Sim 0.108961\n",
      "[00:05:47] Epoch 77: G 82.090698, D 2.302492 Dur 0.000183, Sim 0.064490\n",
      "[00:05:51] Epoch 78: G 81.328766, D 2.261900 Dur 0.000238, Sim 0.066904\n",
      "[00:05:56] Epoch 79: G 81.379150, D 2.307710 Dur 0.000256, Sim 0.058095\n",
      "[00:06:00] Epoch 80: G 81.425140, D 2.341204 Dur 0.000210, Sim 0.069321\n",
      "[00:06:05] Epoch 81: G 81.267593, D 2.203420 Dur 0.000103, Sim 0.059137\n",
      "[00:06:09] Epoch 82: G 81.609726, D 2.209383 Dur 0.000024, Sim 0.064696\n",
      "[00:06:14] Epoch 83: G 81.201950, D 2.074955 Dur 0.000032, Sim 0.081715\n",
      "[00:06:18] Epoch 84: G 81.453735, D 2.127514 Dur 0.000097, Sim 0.063583\n",
      "[00:06:23] Epoch 85: G 81.639168, D 2.164367 Dur 0.000118, Sim 0.064992\n",
      "[00:06:27] Epoch 86: G 81.787979, D 2.102707 Dur 0.000085, Sim 0.081946\n",
      "[00:06:32] Epoch 87: G 82.105042, D 2.146122 Dur 0.000029, Sim 0.059918\n",
      "[00:06:36] Epoch 88: G 81.725365, D 2.147419 Dur 0.000011, Sim 0.060717\n",
      "[00:06:40] Epoch 89: G 81.116684, D 2.170904 Dur 0.000018, Sim 0.088276\n",
      "[00:06:45] Epoch 90: G 81.535202, D 2.302852 Dur 0.000017, Sim 0.094452\n",
      "[00:06:49] Epoch 91: G 81.745552, D 2.219158 Dur 0.000014, Sim 0.076270\n",
      "[00:06:54] Epoch 92: G 81.277969, D 2.273751 Dur 0.000012, Sim 0.101333\n",
      "[00:06:58] Epoch 93: G 81.533653, D 2.187196 Dur 0.000011, Sim 0.080785\n",
      "[00:07:03] Epoch 94: G 81.740372, D 2.169617 Dur 0.000011, Sim 0.089800\n",
      "[00:07:07] Epoch 95: G 80.945496, D 2.209272 Dur 0.000009, Sim 0.095110\n",
      "[00:07:12] Epoch 96: G 81.347733, D 2.202060 Dur 0.000010, Sim 0.079362\n",
      "[00:07:16] Epoch 97: G 81.210632, D 2.214403 Dur 0.000010, Sim 0.070987\n",
      "[00:07:21] Epoch 98: G 81.758751, D 2.194934 Dur 0.000009, Sim 0.096058\n",
      "[00:07:25] Epoch 99: G 81.742088, D 2.114274 Dur 0.000009, Sim 0.085882\n",
      "[00:07:30] Epoch 100: G 81.649979, D 2.112660 Dur 0.000010, Sim 0.102905\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from train import train\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, hps.train.epochs + 1):\n",
    "    loss = train(\n",
    "        hps,\n",
    "        [net_g, net_d, net_dur_disc, net_wd, wl],\n",
    "        [optim_g, optim_d, optim_dur_disc, optim_wd],\n",
    "        train_loader,\n",
    "        scaler,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    loss_gen_all, loss_disc_all, loss_dur_disc_all, loss_slm = loss\n",
    "    \n",
    "    scheduler_g.step()\n",
    "    scheduler_d.step()\n",
    "    scheduler_wd.step()\n",
    "    scheduler_dur_disc.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    hours, remainder = divmod(duration, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    formatted_time = \"{:02d}:{:02d}:{:02d}\".format(int(hours), int(minutes), int(seconds))\n",
    "    print(f\"[{formatted_time}] Epoch {epoch}: G {loss_gen_all:.6f}, D {loss_disc_all:.6f} \"\n",
    "            f\"Dur {loss_dur_disc_all:.6f}, Sim {loss_slm:.6f}\")\n",
    "\n",
    "    if epoch % 10 == 0 and mode == \"clean\":  # Intermediate checkpoints for SEP mode\n",
    "        save_path = f\"checkpoints/{dataset_name}/clean/{model_name}_{mode}_{dataset_name}_{epoch}.pth\"\n",
    "        os.makedirs(f\"checkpoints/{dataset_name}/clean\", exist_ok=True)\n",
    "        torch.save(net_g.state_dict(), save_path)\n",
    "\n",
    "os.makedirs(f\"checkpoints/{dataset_name}\", exist_ok=True)\n",
    "save_path = f\"checkpoints/{dataset_name}/{model_name}_{mode}_{dataset_name}_{epoch}.pth\"\n",
    "torch.save(net_g.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]100%|██████████| 26/26 [00:07<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode SPEC, MCD:  {14.771323384971254}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:49<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode SPEC: GT WER is 0.000000, Syn WER is 0.996104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:01<00:00, 24.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode SPEC on LibriTTS, SIM 0.204721, ASR 0.23076923.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluation\n",
    "\n",
    "evaluation(hps.data.testing_files, net_g, model_name, dataset_name, mode, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
